{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8421b209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def histogram_matching(source_image, target_image):\n",
    "    # Load the source and target images\n",
    "    source = cv2.imread(source_image)\n",
    "    target = cv2.imread(target_image)\n",
    "\n",
    "    # Check dimensions and resize if necessary\n",
    "    if source.shape != target.shape:\n",
    "        target = cv2.resize(target, (source.shape[1], source.shape[0]))\n",
    "\n",
    "    # Convert images from BGR to LAB color space\n",
    "    source_lab = cv2.cvtColor(source, cv2.COLOR_BGR2LAB)\n",
    "    target_lab = cv2.cvtColor(target, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Split the LAB images into individual channels\n",
    "    source_l, source_a, source_b = cv2.split(source_lab)\n",
    "    target_l, target_a, target_b = cv2.split(target_lab)\n",
    "\n",
    "    # Calculate the cumulative histograms for the source and target images\n",
    "    source_hist, _ = np.histogram(source_l.flatten(), bins=256, range=[0, 256], density=True)\n",
    "    target_hist, _ = np.histogram(target_l.flatten(), bins=256, range=[0, 256], density=True)\n",
    "    source_cumulative_hist = np.cumsum(source_hist)\n",
    "    target_cumulative_hist = np.cumsum(target_hist)\n",
    "\n",
    "    # Perform histogram matching by mapping the source cumulative histogram to the target cumulative histogram\n",
    "    transfer_l = np.interp(source_l.flatten().astype(float), source_cumulative_hist, target_cumulative_hist).reshape(source_l.shape)\n",
    "\n",
    "    # Merge the transfered L channel with the target A and B channels\n",
    "    transfer_lab = cv2.merge((transfer_l.astype(np.uint8), target_a, target_b))\n",
    "\n",
    "    # Convert the LAB image back to BGR format\n",
    "    transfer_bgr = cv2.cvtColor(transfer_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return transfer_bgr\n",
    "\n",
    "# Example usage\n",
    "source_image_path = r\"C:\\Users\\MAHESH\\Desktop\\result image\\original.jpg\"\n",
    "target_image_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\SRFP DAILY REPORTS\\Enhanced-power law300.jpg\"\n",
    "\n",
    "result = histogram_matching(source_image_path, target_image_path)\n",
    "\n",
    "# Display and save the result\n",
    "cv2.imshow('Histogram Matching Result', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.imwrite('result_image.jpg', result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c84e4f84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def color_transfer(source_image_path, target_image_path):\n",
    "    # Load the source and target images\n",
    "    source_image = cv2.imread(source_image_path)\n",
    "    target_image = cv2.imread(target_image_path)\n",
    "\n",
    "    # Resize the source image to match the dimensions of the target image\n",
    "    source_image = cv2.resize(source_image, (target_image.shape[1], target_image.shape[0]))\n",
    "\n",
    "    # Convert images to Lab color space\n",
    "    source_lab = cv2.cvtColor(source_image, cv2.COLOR_BGR2LAB)\n",
    "    target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Extract the L channel from the Lab images\n",
    "    source_l = source_lab[:,:,0]\n",
    "    target_l = target_lab[:,:,0]\n",
    "\n",
    "    # Perform color transfer on the L channel\n",
    "    transfer_l = np.interp(source_l.flatten(), (0, 255), (0, 255)).reshape(source_l.shape)\n",
    "\n",
    "    # Merge the transfered L channel with the target A and B channels\n",
    "    transfer_lab = cv2.merge((transfer_l.astype(np.uint8), target_lab[:,:,1], target_lab[:,:,2]))\n",
    "\n",
    "    # Convert the LAB image back to BGR format\n",
    "    transfer_bgr = cv2.cvtColor(transfer_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return transfer_bgr\n",
    "\n",
    "# Example usage\n",
    "source_image_path =r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\FACE MATCHING DATA SET\\DSC_2488.JPG\"\n",
    "target_image_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\FACE MATCHING DATA SET\\DSC_2499.JPG\"\n",
    "result = color_transfer(source_image_path, target_image_path)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Color Transfer Result', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save the result\n",
    "output_path = r\"C:\\Users\\MAHESH\\Desktop\\result image\\color_transfer_result.jpg\"\n",
    "cv2.imwrite(output_path, result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428e4990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def color_transfer(source_image_path, target_image_path):\n",
    "    # Load the source and target images\n",
    "    source_image = cv2.imread(source_image_path)\n",
    "    target_image = cv2.imread(target_image_path)\n",
    "\n",
    "    # Resize the source image to match the dimensions of the target image\n",
    "    source_image = cv2.resize(source_image, (target_image.shape[1], target_image.shape[0]))\n",
    "\n",
    "    # Convert images to Lab color space\n",
    "    source_lab = cv2.cvtColor(source_image, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "    target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "\n",
    "    # Compute the mean and standard deviation of the L, A, and B channels for source and target images\n",
    "    source_mean, source_std = cv2.meanStdDev(source_lab)\n",
    "    target_mean, target_std = cv2.meanStdDev(target_lab)\n",
    "\n",
    "    # Perform color transfer by adjusting the mean and standard deviation of the L, A, and B channels\n",
    "    transfer_lab = np.zeros_like(source_lab)\n",
    "    for i in range(3):\n",
    "        transfer_lab[:,:,i] = (source_lab[:,:,i] - source_mean[i]) * (target_std[i] / source_std[i]) + target_mean[i]\n",
    "\n",
    "    # Convert the LAB image back to BGR format\n",
    "    transfer_bgr = cv2.cvtColor(transfer_lab.astype(np.uint8), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return transfer_bgr\n",
    "\n",
    "# Example usage\n",
    "source_image_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\FACE MATCHING DATA SET\\DSC_2499.JPG\"\n",
    "target_image_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\FACE MATCHING DATA SET\\DSC_2499.JPG\"\n",
    "result = color_transfer(source_image_path, target_image_path)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Color Transfer Result', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save the result\n",
    "output_path = r\"C:\\Users\\MAHESH\\Desktop\\result image\\color_transfer_result.jpg\"\n",
    "cv2.imwrite(output_path, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a35675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def color_transfer(source_images, target_image):\n",
    "    # Convert images to Lab color space\n",
    "    source_labs = [cv2.cvtColor(img, cv2.COLOR_BGR2LAB).astype(np.float32) for img in source_images]\n",
    "    target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "\n",
    "    # Compute the mean and standard deviation of the L, A, and B channels for source and target images\n",
    "    source_means, source_stds = zip(*[cv2.meanStdDev(src_lab) for src_lab in source_labs])\n",
    "    target_mean, target_std = cv2.meanStdDev(target_lab)\n",
    "\n",
    "    # Perform color transfer on each source image by adjusting the mean and standard deviation\n",
    "    transfer_labs = []\n",
    "    for i in range(len(source_images)):\n",
    "        transfer_lab = np.zeros_like(source_labs[i])\n",
    "        for j in range(3):\n",
    "            transfer_lab[:,:,j] = (source_labs[i][:,:,j] - source_means[i][j]) * (target_std[j] / source_stds[i][j]) + target_mean[j]\n",
    "        transfer_labs.append(transfer_lab)\n",
    "\n",
    "    # Convert the LAB images back to BGR format\n",
    "    transfer_bgrs = [cv2.cvtColor(transfer_lab.astype(np.uint8), cv2.COLOR_LAB2BGR) for transfer_lab in transfer_labs]\n",
    "\n",
    "    return transfer_bgrs\n",
    "\n",
    "# Example usage\n",
    "source_image_paths = [\n",
    "    r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\FACE MATCHING DATA SET\\DSC_2498.JPG\",\n",
    "    r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\FACE MATCHING DATA SET\\DSC_2494.JPG\",\n",
    "    r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\FACE MATCHING DATA SET\\DSC_2488.JPG\",\n",
    "    r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\FACE MATCHING DATA SET\\DSC_2492.JPG\"\n",
    "]\n",
    "target_image_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\FACE MATCHING DATA SET\\DSC_2499.JPG\"\n",
    "\n",
    "# Load the source and target images\n",
    "source_images = [cv2.imread(path) for path in source_image_paths]\n",
    "target_image = cv2.imread(target_image_path)\n",
    "\n",
    "# Perform color transfer on the source images using the target image\n",
    "transfer_results = color_transfer(source_images, target_image)\n",
    "\n",
    "# Display the results\n",
    "for i, result in enumerate(transfer_results):\n",
    "    cv2.imshow(f'Color Transfer Result {i+1}', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save the results\n",
    "output_folder = r\"C:\\Users\\MAHESH\\Desktop\\result image\"\n",
    "for i, result in enumerate(transfer_results):\n",
    "    output_path = f\"{output_folder}\\color_transfer_result_{i+1}.jpg\"\n",
    "    cv2.imwrite(output_path, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2094d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW ERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "906886f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:692: error: (-2:Unspecified error) could not find a writer for the specified extension in function 'cv::imwrite_'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 128>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    125\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m    127\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMAHESH\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSRFP INTERNSHIP\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtrain_in_1\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msaturation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 128\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_enhanced\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:692: error: (-2:Unspecified error) could not find a writer for the specified extension in function 'cv::imwrite_'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def color_transfer(source_image, target_image):\n",
    "    # Convert images to Lab color space\n",
    "    source_lab = cv2.cvtColor(source_image, cv2.COLOR_BGR2LAB)\n",
    "    target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Extract the L, A, B channels from the Lab images\n",
    "    source_l, source_a, source_b = cv2.split(source_lab)\n",
    "    target_l, target_a, target_b = cv2.split(target_lab)\n",
    "\n",
    "    # Perform color transfer on the L channel\n",
    "    transfer_l = np.interp(source_l.flatten(), (0, 255), (0, 255)).reshape(source_l.shape)\n",
    "\n",
    "    # Merge the transfered L channel with the target A and B channels\n",
    "    transfer_lab = cv2.merge((transfer_l.astype(np.uint8), target_a, target_b))\n",
    "\n",
    "    # Convert the LAB image back to BGR format\n",
    "    transfer_bgr = cv2.cvtColor(transfer_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return transfer_bgr\n",
    "\n",
    "\n",
    "def adjust_brightness_exposure(source_image, target_image):\n",
    "    # Convert images to LAB color space\n",
    "    source_lab = cv2.cvtColor(source_image, cv2.COLOR_BGR2LAB)\n",
    "    target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Extract the L channel from the LAB images\n",
    "    source_l = source_lab[:, :, 0]\n",
    "    target_l = target_lab[:, :, 0]\n",
    "\n",
    "    # Calculate the mean difference in brightness\n",
    "    mean_diff = np.mean(source_l) - np.mean(target_l)\n",
    "\n",
    "    # Adjust the brightness/exposure of the target image\n",
    "    adjusted_l = target_l + mean_diff\n",
    "    adjusted_l = np.clip(adjusted_l, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Merge the adjusted L channel with the A and B channels\n",
    "    adjusted_lab = cv2.merge((adjusted_l, target_lab[:, :, 1], target_lab[:, :, 2]))\n",
    "\n",
    "    # Convert the LAB image back to BGR format\n",
    "    adjusted_bgr = cv2.cvtColor(adjusted_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return adjusted_bgr\n",
    "\n",
    "\n",
    "def adjust_saturation(source_image, target_image):\n",
    "    # Convert images to HSV color space\n",
    "    source_hsv = cv2.cvtColor(source_image, cv2.COLOR_BGR2HSV)\n",
    "    target_hsv = cv2.cvtColor(target_image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Extract the S channel from the HSV images\n",
    "    source_s = source_hsv[:, :, 1]\n",
    "    target_s = target_hsv[:, :, 1]\n",
    "\n",
    "    # Calculate the mean difference in saturation\n",
    "    mean_diff = np.mean(source_s) - np.mean(target_s)\n",
    "\n",
    "    # Adjust the saturation of the target image\n",
    "    adjusted_s = target_s + mean_diff\n",
    "    adjusted_s = np.clip(adjusted_s, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Merge the adjusted S channel with the H and V channels\n",
    "    adjusted_hsv = cv2.merge((target_hsv[:, :, 0], adjusted_s, target_hsv[:, :, 2]))\n",
    "\n",
    "    # Convert the HSV image back to BGR format\n",
    "    adjusted_bgr = cv2.cvtColor(adjusted_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return adjusted_bgr\n",
    "\n",
    "\n",
    "def enhance_contrast(source_image, target_image):\n",
    "    # Convert images to LAB color space\n",
    "    source_lab = cv2.cvtColor(source_image, cv2.COLOR_BGR2LAB)\n",
    "    target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Extract the L channel from the LAB images\n",
    "    source_l = source_lab[:, :, 0]\n",
    "    target_l = target_lab[:, :, 0]\n",
    "\n",
    "    # Calculate the mean and standard deviation of the L channel\n",
    "    source_mean, source_std = np.mean(source_l), np.std(source_l)\n",
    "    target_mean, target_std = np.mean(target_l), np.std(target_l)\n",
    "\n",
    "    # Calculate the contrast ratio\n",
    "    contrast_ratio = (source_std + 1) / (target_std + 1)\n",
    "\n",
    "    # Adjust the contrast of the target image\n",
    "    adjusted_l = (target_l - target_mean) * contrast_ratio + source_mean\n",
    "    adjusted_l = np.clip(adjusted_l, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Merge the adjusted L channel with the A and B channels\n",
    "    adjusted_lab = cv2.merge((adjusted_l, target_lab[:, :, 1], target_lab[:, :, 2]))\n",
    "\n",
    "    # Convert the LAB image back to BGR format\n",
    "    adjusted_bgr = cv2.cvtColor(adjusted_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return adjusted_bgr\n",
    "\n",
    "\n",
    "# Load the input and output images\n",
    "input_image1 = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2478.JPG\")\n",
    "input_image2 = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2479.JPG\")\n",
    "target_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Done\\DSC_2480.jpg\")\n",
    "\n",
    "# Perform color transfer from input_image1 to input_image2\n",
    "color_transferred = color_transfer(input_image1, input_image2)\n",
    "\n",
    "# Adjust brightness/exposure of color_transferred image to match target_image\n",
    "brightness_matched = adjust_brightness_exposure(target_image, color_transferred)\n",
    "\n",
    "# Adjust saturation of brightness_matched image to match target_image\n",
    "saturation_adjusted = adjust_saturation(target_image, brightness_matched)\n",
    "\n",
    "# Enhance contrast of saturation_adjusted image to match target_image\n",
    "contrast_enhanced = enhance_contrast(target_image, saturation_adjusted)\n",
    "\n",
    "# Display and save the result\n",
    "cv2.imshow(\"Color Transfer Result\", contrast_enhanced)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "output_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\saturation\"\n",
    "cv2.imwrite(output_path, contrast_enhanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "796bef91",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\arithm.cpp:650: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'cv::arithm_op'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 133>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Apply vignette effect\u001b[39;00m\n\u001b[0;32m    132\u001b[0m vignette_intensity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.6\u001b[39m  \u001b[38;5;66;03m# Adjust the intensity of the vignette effect as desired\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m vignette_result \u001b[38;5;241m=\u001b[39m \u001b[43mapply_vignette\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvignette_intensity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[0;32m    136\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColor Transfer Result\u001b[39m\u001b[38;5;124m\"\u001b[39m, color_transfer_result)\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mapply_vignette\u001b[1;34m(image, intensity)\u001b[0m\n\u001b[0;32m    102\u001b[0m vignette \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(vignette, (width, height))\n\u001b[0;32m    103\u001b[0m vignette \u001b[38;5;241m=\u001b[39m vignette \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(vignette)\n\u001b[1;32m--> 104\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvignette\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\arithm.cpp:650: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'cv::arithm_op'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def color_transfer(source_image, target_image):\n",
    "    # Convert images to LAB color space\n",
    "    source_lab = cv2.cvtColor(source_image, cv2.COLOR_BGR2LAB)\n",
    "    target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Extract the L channel from the LAB images\n",
    "    source_l = source_lab[:, :, 0]\n",
    "    target_l = target_lab[:, :, 0]\n",
    "\n",
    "    # Perform histogram matching to enhance contrast\n",
    "    transfer_l = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)).apply(source_l)\n",
    "    transfer_lab = cv2.merge((transfer_l, target_lab[:, :, 1], target_lab[:, :, 2]))\n",
    "\n",
    "    # Convert the LAB image back to BGR format\n",
    "    transfer_bgr = cv2.cvtColor(transfer_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return transfer_bgr\n",
    "\n",
    "def adjust_brightness_exposure(source_image, target_image):\n",
    "    # Calculate the mean brightness/exposure difference\n",
    "    mean_diff = np.mean(source_image) - np.mean(target_image)\n",
    "\n",
    "    # Adjust the brightness/exposure of the target image\n",
    "    adjusted_image = target_image + mean_diff\n",
    "\n",
    "    return adjusted_image\n",
    "\n",
    "def adjust_saturation(source_image, target_image):\n",
    "    # Convert images to HSV color space\n",
    "    source_hsv = cv2.cvtColor(source_image, cv2.COLOR_BGR2HSV)\n",
    "    target_hsv = cv2.cvtColor(target_image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Adjust the saturation of the target image\n",
    "    target_hsv[:, :, 1] = source_hsv[:, :, 1]\n",
    "\n",
    "    # Convert the HSV image back to BGR format\n",
    "    adjusted_bgr = cv2.cvtColor(target_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return adjusted_bgr\n",
    "\n",
    "def enhance_contrast(source_image, target_image):\n",
    "    # Convert images to LAB color space\n",
    "    source_lab = cv2.cvtColor(source_image, cv2.COLOR_BGR2LAB)\n",
    "    target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Extract the L channel from the LAB images\n",
    "    source_l = source_lab[:, :, 0]\n",
    "    target_l = target_lab[:, :, 0]\n",
    "\n",
    "    # Perform histogram matching to enhance contrast\n",
    "    transfer_l = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)).apply(source_l)\n",
    "    transfer_lab = cv2.merge((transfer_l, target_lab[:, :, 1], target_lab[:, :, 2]))\n",
    "\n",
    "    # Convert the LAB image back to BGR format\n",
    "    transfer_bgr = cv2.cvtColor(transfer_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return transfer_bgr\n",
    "\n",
    "def adjust_tint(source_image, target_image):\n",
    "    # Calculate the mean difference in color channels\n",
    "    mean_diff_b = np.mean(source_image[:, :, 0]) - np.mean(target_image[:, :, 0])\n",
    "    mean_diff_g = np.mean(source_image[:, :, 1]) - np.mean(target_image[:, :, 1])\n",
    "    mean_diff_r = np.mean(source_image[:, :, 2]) - np.mean(target_image[:, :, 2])\n",
    "\n",
    "    # Adjust the tint of the target image\n",
    "    adjusted_b = target_image[:, :, 0] + mean_diff_b\n",
    "    adjusted_g = target_image[:, :, 1] + mean_diff_g\n",
    "    adjusted_r = target_image[:, :, 2] + mean_diff_r\n",
    "\n",
    "    # Merge the adjusted color channels\n",
    "    adjusted_bgr = cv2.merge((adjusted_b, adjusted_g, adjusted_r))\n",
    "\n",
    "    return adjusted_bgr\n",
    "\n",
    "def adjust_temperature(source_image, target_image, temperature):\n",
    "    # Convert images to LAB color space\n",
    "    source_lab = cv2.cvtColor(source_image, cv2.COLOR_BGR2LAB)\n",
    "    target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Adjust the A channel of the target image based on temperature difference\n",
    "    target_lab[:, :, 1] += temperature\n",
    "\n",
    "    # Convert the LAB image back to BGR format\n",
    "    adjusted_bgr = cv2.cvtColor(target_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return adjusted_bgr\n",
    "\n",
    "def apply_vignette(image, intensity):\n",
    "    # Create a circular mask for vignette effect\n",
    "    height, width = image.shape[:2]\n",
    "    center = (width // 2, height // 2)\n",
    "    radius = int(min(width, height) * 0.7)\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    cv2.circle(mask, center, radius, 255, -1)\n",
    "\n",
    "    # Apply vignette effect to the image\n",
    "    vignette = cv2.getGaussianKernel(width, intensity * width)\n",
    "    vignette = vignette * vignette.T\n",
    "    vignette = cv2.resize(vignette, (width, height))\n",
    "    vignette = vignette / np.max(vignette)\n",
    "    result = cv2.multiply(image, vignette.astype(image.dtype))\n",
    "\n",
    "    return result\n",
    "\n",
    "# Load the input and target images\n",
    "input_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\FACE MATCHING DATA SET\\DSC_2498.JPG\")\n",
    "target_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\FACE MATCHING DATA SET\\DSC_2499.JPG\")\n",
    "\n",
    "# Color transfer\n",
    "color_transfer_result = color_transfer(input_image, target_image)\n",
    "\n",
    "# Adjust brightness/exposure\n",
    "brightness_exposure_result = adjust_brightness_exposure(input_image, target_image)\n",
    "\n",
    "# Adjust saturation\n",
    "saturation_result = adjust_saturation(input_image, target_image)\n",
    "\n",
    "# Enhance contrast\n",
    "contrast_result = enhance_contrast(input_image, target_image)\n",
    "\n",
    "# Adjust tint\n",
    "tint_result = adjust_tint(input_image, target_image)\n",
    "\n",
    "# Adjust temperature\n",
    "temperature = 10  # Adjust the temperature value as desired\n",
    "temperature_result = adjust_temperature(input_image, target_image, temperature)\n",
    "\n",
    "# Apply vignette effect\n",
    "vignette_intensity = 0.6  # Adjust the intensity of the vignette effect as desired\n",
    "vignette_result = apply_vignette(input_image, vignette_intensity)\n",
    "\n",
    "# Display the results\n",
    "cv2.imshow(\"Color Transfer Result\", color_transfer_result)\n",
    "cv2.imshow(\"Brightness/Exposure Result\", brightness_exposure_result)\n",
    "cv2.imshow(\"Saturation Result\", saturation_result)\n",
    "cv2.imshow(\"Contrast Result\", contrast_result)\n",
    "cv2.imshow(\"Tint Result\", tint_result)\n",
    "cv2.imshow(\"Temperature Result\", temperature_result)\n",
    "cv2.imshow(\"Vignette Result\", vignette_result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f04493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def equalize_channels(image, num_channels):\n",
    "    # Check the number of channels\n",
    "    num_image_channels = image.shape[2]\n",
    "    \n",
    "    if num_image_channels == num_channels:\n",
    "        return image\n",
    "    elif num_image_channels == 1:\n",
    "        return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif num_image_channels == 3:\n",
    "        return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid number of image channels: {}. Expected 1 or 3.\".format(num_image_channels))\n",
    "\n",
    "def resize_image(image, target_size):\n",
    "    # Resize the image to the target size\n",
    "    resized_image = cv2.resize(image, target_size)\n",
    "    return resized_image\n",
    "\n",
    "def apply_brightness(image, brightness):\n",
    "    # Adjust the brightness/exposure of the image\n",
    "    adjusted_image = cv2.convertScaleAbs(image, alpha=brightness)\n",
    "    return adjusted_image\n",
    "\n",
    "def apply_contrast(image, contrast):\n",
    "    # Adjust the contrast of the image\n",
    "    adjusted_image = cv2.convertScaleAbs(image, alpha=contrast)\n",
    "    return adjusted_image\n",
    "\n",
    "def apply_saturation(image, saturation):\n",
    "    # Convert the image to the HSV color space\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # Scale the saturation channel\n",
    "    hsv_image[..., 1] = np.clip(hsv_image[..., 1] * saturation, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Convert the image back to the RGB color space\n",
    "    adjusted_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2RGB)\n",
    "    return adjusted_image\n",
    "\n",
    "def apply_temperature(image, temperature):\n",
    "    # Convert the image to the LAB color space\n",
    "    lab_image = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "\n",
    "    # Adjust the A channel of the LAB image based on the temperature\n",
    "    lab_image[..., 1] = np.clip(lab_image[..., 1] + temperature, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Convert the image back to the RGB color space\n",
    "    adjusted_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2RGB)\n",
    "    return adjusted_image\n",
    "\n",
    "def apply_vignette(image, intensity):\n",
    "    # Create a vignette mask\n",
    "    height, width = image.shape[:2]\n",
    "    mask = np.zeros((height, width), dtype=np.float32)\n",
    "    cv2.circle(mask, (int(width / 2), int(height / 2)), int(min(width, height) * 0.7), 1, -1)\n",
    "\n",
    "    # Normalize the mask\n",
    "    mask = mask / np.max(mask)\n",
    "\n",
    "    # Apply the vignette effect\n",
    "    result = cv2.multiply(image, mask[:, :, np.newaxis])\n",
    "\n",
    "    # Adjust the intensity of the vignette effect\n",
    "    result = cv2.multiply(result, intensity)\n",
    "    return result\n",
    "\n",
    "def apply_tint(image, tint_color):\n",
    "    # Convert the image to the LAB color space\n",
    "    lab_image = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "\n",
    "    # Adjust the B channel of the LAB image based on the tint color\n",
    "    lab_image[..., 2] = np.clip(lab_image[..., 2] + tint_color, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Convert the image back to the RGB color space\n",
    "    adjusted_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2RGB)\n",
    "    return adjusted_image\n",
    "\n",
    "def color_transfer(input_image_path, target_image_path):\n",
    "    # Load the input and target images\n",
    "    input_image = cv2.imread(input_image_path)\n",
    "    target_image = cv2.imread(target_image_path)\n",
    "\n",
    "    # Equalize the number of channels in the images\n",
    "    num_channels = 3  # Set the desired number of channels (e.g., 1 for grayscale or 3 for RGB)\n",
    "    input_image = equalize_channels(input_image, num_channels)\n",
    "    target_image = equalize_channels(target_image, num_channels)\n",
    "\n",
    "    # Resize the input image to match the size of the target image\n",
    "    input_image = resize_image(input_image, (target_image.shape[1], target_image.shape[0]))\n",
    "\n",
    "    # Perform color transfer and adjustments\n",
    "    # ...\n",
    "\n",
    "    # Display the results\n",
    "    cv2.imshow(\"Input Image\", input_image)\n",
    "    cv2.imshow(\"Target Image\", target_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Save the results\n",
    "    # ...\n",
    "\n",
    "# Example usage\n",
    "input_image_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\FACE MATCHING DATA SET\\DSC_2498.JPG\"\n",
    "target_image_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\FACE MATCHING DATA SET\\DSC_2499.JPG\"\n",
    "color_transfer(input_image_path, target_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b0f1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def resize_image(image, target_size):\n",
    "    # Resize the image while maintaining the aspect ratio\n",
    "    height, width = image.shape[:2]\n",
    "    target_width, target_height = target_size\n",
    "\n",
    "    # Calculate the aspect ratios\n",
    "    image_ratio = width / height\n",
    "    target_ratio = target_width / target_height\n",
    "\n",
    "    # Determine the new dimensions\n",
    "    if target_ratio < image_ratio:\n",
    "        new_width = target_width\n",
    "        new_height = int(new_width / image_ratio)\n",
    "    else:\n",
    "        new_height = target_height\n",
    "        new_width = int(new_height * image_ratio)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "    return resized_image\n",
    "\n",
    "def color_transfer(source_image, target_image):\n",
    "    # Convert images to LAB color space\n",
    "    source_lab = cv2.cvtColor(source_image, cv2.COLOR_BGR2LAB)\n",
    "    target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Extract the L channel from the LAB images\n",
    "    source_l = source_lab[:, :, 0]\n",
    "    target_l = target_lab[:, :, 0]\n",
    "\n",
    "    # Perform histogram matching to enhance contrast\n",
    "    transfer_l = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)).apply(source_l)\n",
    "    transfer_lab = cv2.merge((transfer_l, target_lab[:, :, 1], target_lab[:, :, 2]))\n",
    "\n",
    "    # Convert the LAB image back to BGR format\n",
    "    transfer_bgr = cv2.cvtColor(transfer_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return transfer_bgr\n",
    "\n",
    "def adjust_brightness_exposure(source_image, target_image):\n",
    "    # Calculate the mean brightness/exposure difference\n",
    "    mean_diff = np.mean(source_image) - np.mean(target_image)\n",
    "\n",
    "    # Adjust the brightness/exposure of the target image\n",
    "    adjusted_image = target_image + mean_diff\n",
    "\n",
    "    return adjusted_image\n",
    "\n",
    "def adjust_saturation(source_image, target_image):\n",
    "    # Convert images to HSV color space\n",
    "    source_hsv = cv2.cvtColor(source_image, cv2.COLOR_BGR2HSV)\n",
    "    target_hsv = cv2.cvtColor(target_image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Adjust the saturation of the target image\n",
    "    target_hsv[:, :, 1] = source_hsv[:, :, 1]\n",
    "\n",
    "    # Convert the HSV image back to BGR format\n",
    "    adjusted_bgr = cv2.cvtColor(target_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return adjusted_bgr\n",
    "\n",
    "def enhance_contrast(source_image, target_image):\n",
    "    # Convert images to LAB color space\n",
    "    source_lab = cv2.cvtColor(source_image, cv2.COLOR_BGR2LAB)\n",
    "    target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Extract the L channel from the LAB images\n",
    "    source_l = source_lab[:, :, 0]\n",
    "    target_l = target_lab[:, :, 0]\n",
    "\n",
    "    # Perform histogram matching to enhance contrast\n",
    "    transfer_l = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)).apply(source_l)\n",
    "    transfer_lab = cv2.merge((transfer_l, target_lab[:, :, 1], target_lab[:, :, 2]))\n",
    "\n",
    "    # Convert the LAB image back to BGR format\n",
    "    transfer_bgr = cv2.cvtColor(transfer_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return transfer_bgr\n",
    "\n",
    "def adjust_tint(source_image, target_image):\n",
    "    # Calculate the mean difference in color channels\n",
    "    mean_diff_b = np.mean(source_image[:, :, 0]) - np.mean(target_image[:, :, 0])\n",
    "    mean_diff_g = np.mean(source_image[:, :, 1]) - np.mean(target_image[:, :, 1])\n",
    "    mean_diff_r = np.mean(source_image[:, :, 2]) - np.mean(target_image[:, :, 2])\n",
    "\n",
    "    # Adjust the tint of the target image\n",
    "    adjusted_b = target_image[:, :, 0] + mean_diff_b\n",
    "    adjusted_g = target_image[:, :, 1] + mean_diff_g\n",
    "    adjusted_r = target_image[:, :, 2] + mean_diff_r\n",
    "\n",
    "    adjusted_bgr = np.clip(np.stack((adjusted_b, adjusted_g, adjusted_r), axis=-1), 0, 255).astype(np.uint8)\n",
    "\n",
    "    return adjusted_bgr\n",
    "\n",
    "def adjust_temperature(source_image, target_image, temperature):\n",
    "    # Convert images to LAB color space\n",
    "    source_lab = cv2.cvtColor(source_image, cv2.COLOR_BGR2LAB)\n",
    "    target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Adjust the A channel of the target image based on temperature difference\n",
    "    target_lab[:, :, 1] += temperature\n",
    "\n",
    "    # Convert the LAB image back to BGR format\n",
    "    adjusted_bgr = cv2.cvtColor(target_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return adjusted_bgr\n",
    "\n",
    "def apply_vignette(image, intensity):\n",
    "    # Create a circular mask for vignette effect\n",
    "    height, width = image.shape[:2]\n",
    "    center = (width // 2, height // 2)\n",
    "    radius = int(min(width, height) * 0.7)\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    cv2.circle(mask, center, radius, 255, -1)\n",
    "\n",
    "    # Apply vignette effect to the image\n",
    "    vignette = cv2.getGaussianKernel(width, intensity * width)\n",
    "    vignette = vignette * vignette.T\n",
    "    vignette = cv2.resize(vignette, (width, height))\n",
    "    vignette = vignette / np.max(vignette)\n",
    "\n",
    "    # Apply the vignette mask to the image\n",
    "    result = image * vignette[..., np.newaxis]  # Multiply each channel with the vignette mask\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# Provide the paths or variables for the input and target images\n",
    "input_image_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2577.JPG\"\n",
    "target_image_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2566.JPG\"\n",
    "\n",
    "# Load the input and target images\n",
    "input_image = cv2.imread(input_image_path)\n",
    "target_image = cv2.imread(target_image_path)\n",
    "\n",
    "# Check if the input and target images have the same number of channels\n",
    "if input_image.shape[2] != target_image.shape[2]:\n",
    "    raise ValueError(\"The input and target images must have the same number of channels.\")\n",
    "\n",
    "# Resize the input image to match the size of the target image\n",
    "input_image = resize_image(input_image, (target_image.shape[1], target_image.shape[0]))\n",
    "\n",
    "# Color transfer\n",
    "color_transfer_result = color_transfer(input_image, target_image)\n",
    "\n",
    "# Adjust brightness/exposure\n",
    "brightness_exposure_result = adjust_brightness_exposure(input_image, target_image)\n",
    "\n",
    "# Adjust saturation\n",
    "saturation_result = adjust_saturation(input_image, target_image)\n",
    "\n",
    "# Enhance contrast\n",
    "contrast_result = enhance_contrast(input_image, target_image)\n",
    "\n",
    "# Adjust tint\n",
    "tint_result = adjust_tint(input_image, target_image)\n",
    "\n",
    "# Adjust temperature\n",
    "temperature = 10  # Adjust the temperature value as desired\n",
    "temperature_result = adjust_temperature(input_image, target_image, temperature)\n",
    "\n",
    "# Apply vignette effect\n",
    "vignette_intensity = 0.6  # Adjust the intensity of the vignette effect as desired\n",
    "vignette_result = apply_vignette(input_image, vignette_intensity)\n",
    "\n",
    "# Display the results\n",
    "cv2.imshow(\"Color Transfer Result\", color_transfer_result)\n",
    "cv2.imshow(\"Brightness/Exposure Result\", brightness_exposure_result)\n",
    "cv2.imshow(\"Saturation Result\", saturation_result)\n",
    "cv2.imshow(\"Contrast Result\", contrast_result)\n",
    "cv2.imshow(\"Tint Result\", tint_result)\n",
    "cv2.imshow(\"Temperature Result\", temperature_result)\n",
    "cv2.imshow(\"Vignette Result\", vignette_result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "642c4b9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) d:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0x416a7d27::Set<3,-1,-1>,struct cv::impl::A0x416a7d27::Set<0,5,-1>,2>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 127>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    124\u001b[0m brightness_exposure_result \u001b[38;5;241m=\u001b[39m adjust_brightness_exposure(input_image, target_image)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Adjust saturation\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m saturation_result \u001b[38;5;241m=\u001b[39m \u001b[43madjust_saturation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbrightness_exposure_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Enhance contrast\u001b[39;00m\n\u001b[0;32m    130\u001b[0m contrast_result \u001b[38;5;241m=\u001b[39m enhance_contrast(saturation_result, target_image)\n",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36madjust_saturation\u001b[1;34m(source_image, target_image)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madjust_saturation\u001b[39m(source_image, target_image):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# Convert images to HSV color space\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     source_hsv \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2HSV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     target_hsv \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(target_image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2HSV)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Adjust the saturation of the target image\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) d:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0x416a7d27::Set<3,-1,-1>,struct cv::impl::A0x416a7d27::Set<0,5,-1>,2>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def resize_image(image, target_size):\n",
    "    # Resize the image to the target size\n",
    "    resized_image = cv2.resize(image, target_size)\n",
    "    return resized_image\n",
    "\n",
    "def color_transfer(source_image, target_image):\n",
    "    # Convert images to LAB color space\n",
    "    source_lab = cv2.cvtColor(source_image, cv2.COLOR_BGR2LAB)\n",
    "    target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Extract the L channel from the LAB images\n",
    "    source_l = source_lab[:, :, 0]\n",
    "    target_l = target_lab[:, :, 0]\n",
    "\n",
    "    # Perform histogram matching to enhance contrast\n",
    "    transfer_l = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)).apply(source_l)\n",
    "    transfer_lab = cv2.merge((transfer_l, target_lab[:, :, 1], target_lab[:, :, 2]))\n",
    "\n",
    "    # Convert the LAB image back to BGR format\n",
    "    transfer_bgr = cv2.cvtColor(transfer_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return transfer_bgr\n",
    "\n",
    "def adjust_brightness_exposure(source_image, target_image):\n",
    "    # Calculate the mean brightness/exposure difference\n",
    "    mean_diff = np.mean(source_image) - np.mean(target_image)\n",
    "\n",
    "    # Adjust the brightness/exposure of the target image\n",
    "    adjusted_image = target_image + mean_diff\n",
    "\n",
    "    return adjusted_image\n",
    "\n",
    "def adjust_saturation(source_image, target_image):\n",
    "    # Convert images to HSV color space\n",
    "    source_hsv = cv2.cvtColor(source_image, cv2.COLOR_BGR2HSV)\n",
    "    target_hsv = cv2.cvtColor(target_image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Adjust the saturation of the target image\n",
    "    target_hsv[:, :, 1] = source_hsv[:, :, 1]\n",
    "\n",
    "    # Convert the HSV image back to BGR format\n",
    "    adjusted_bgr = cv2.cvtColor(target_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return adjusted_bgr\n",
    "\n",
    "def enhance_contrast(source_image, target_image):\n",
    "    # Convert images to LAB color space\n",
    "    source_lab = cv2.cvtColor(source_image, cv2.COLOR_BGR2LAB)\n",
    "    target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Extract the L channel from the LAB images\n",
    "    source_l = source_lab[:, :, 0]\n",
    "    target_l = target_lab[:, :, 0]\n",
    "\n",
    "    # Perform histogram matching to enhance contrast\n",
    "    transfer_l = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)).apply(source_l)\n",
    "    transfer_lab = cv2.merge((transfer_l, target_lab[:, :, 1], target_lab[:, :, 2]))\n",
    "\n",
    "    # Convert the LAB image back to BGR format\n",
    "    transfer_bgr = cv2.cvtColor(transfer_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return transfer_bgr\n",
    "\n",
    "def adjust_tint(source_image, target_image):\n",
    "    # Calculate the mean difference in color channels\n",
    "    mean_diff_b = np.mean(source_image[:, :, 0]) - np.mean(target_image[:, :, 0])\n",
    "    mean_diff_g = np.mean(source_image[:, :, 1]) - np.mean(target_image[:, :, 1])\n",
    "    mean_diff_r = np.mean(source_image[:, :, 2]) - np.mean(target_image[:, :, 2])\n",
    "\n",
    "    # Adjust the tint of the target image\n",
    "    adjusted_b = target_image[:, :, 0] + mean_diff_b\n",
    "    adjusted_g = target_image[:, :, 1] + mean_diff_g\n",
    "    adjusted_r = target_image[:, :, 2] + mean_diff_r\n",
    "    adjusted_image = cv2.merge((adjusted_b, adjusted_g, adjusted_r))\n",
    "\n",
    "    return adjusted_image\n",
    "\n",
    "def apply_vignette(image, intensity):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Create the vignette mask\n",
    "    height, width = gray.shape\n",
    "    center = (width // 2, height // 2)\n",
    "    max_distance = np.sqrt(center[0]**2 + center[1]**2)\n",
    "    x, y = np.meshgrid(np.arange(width), np.arange(height))\n",
    "    distance = np.sqrt((x - center[0])**2 + (y - center[1])**2)\n",
    "    vignette = 1 - intensity * (distance / max_distance)\n",
    "\n",
    "    # Resize the vignette mask to match the image size\n",
    "    vignette = cv2.resize(vignette, (width, height))\n",
    "\n",
    "    # Apply the vignette effect\n",
    "    result = cv2.multiply(image, vignette.astype(image.dtype))\n",
    "\n",
    "    return result\n",
    "\n",
    "def normalize_depths(source_image, target_image):\n",
    "    # Normalize the depths of the input and target images to the same depth\n",
    "    source_depth = source_image.dtype\n",
    "    target_depth = target_image.dtype\n",
    "\n",
    "    if source_depth != target_depth:\n",
    "        # Convert the source image to the target depth\n",
    "        source_image = source_image.astype(target_depth)\n",
    "\n",
    "    return source_image, target_image\n",
    "\n",
    "# Load the input and target images\n",
    "input_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\FACE MATCHING DATA SET\\DSC_2498.JPG\")\n",
    "target_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\FACE MATCHING DATA SET\\DSC_2499.JPG\")\n",
    "\n",
    "# Resize the input image to match the target image size\n",
    "target_size = (target_image.shape[1], target_image.shape[0])\n",
    "input_image = resize_image(input_image, target_size)\n",
    "\n",
    "# Normalize the depths of the input and target images\n",
    "input_image, target_image = normalize_depths(input_image, target_image)\n",
    "\n",
    "# Adjust brightness/exposure\n",
    "brightness_exposure_result = adjust_brightness_exposure(input_image, target_image)\n",
    "\n",
    "# Adjust saturation\n",
    "saturation_result = adjust_saturation(brightness_exposure_result, target_image)\n",
    "\n",
    "# Enhance contrast\n",
    "contrast_result = enhance_contrast(saturation_result, target_image)\n",
    "\n",
    "# Adjust tint\n",
    "tint_result = adjust_tint(contrast_result, target_image)\n",
    "\n",
    "# Apply vignette effect\n",
    "vignette_intensity = 0.6  # Adjust the intensity of the vignette effect as desired\n",
    "vignette_result = apply_vignette(tint_result, vignette_intensity)\n",
    "\n",
    "# Display the results\n",
    "cv2.imshow(\"Input Image\", input_image)\n",
    "cv2.imshow(\"Target Image\", target_image)\n",
    "cv2.imshow(\"Vignette Result\", vignette_result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1649c8dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\arithm.cpp:652: error: (-215:Assertion failed) type2 == CV_64F && (sz2.height == 1 || sz2.height == 4) in function 'cv::arithm_op'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 120>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    117\u001b[0m target_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMAHESH\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSRFP INTERNSHIP\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtrain_in_1\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFACE MATCHING DATA SET\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDSC_2499.JPG\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Call the color transfer function\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[43mcolor_transfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_image_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36mcolor_transfer\u001b[1;34m(input_image_path, target_image_path)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Tint Adjustment\u001b[39;00m\n\u001b[0;32m     97\u001b[0m tint_color \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Adjust the tint color as desired\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m adjusted_image \u001b[38;5;241m=\u001b[39m \u001b[43madjust_tint\u001b[49m\u001b[43m(\u001b[49m\u001b[43madjusted_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtint_color\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Temperature Adjustment\u001b[39;00m\n\u001b[0;32m    101\u001b[0m adjusted_image \u001b[38;5;241m=\u001b[39m adjust_temperature(adjusted_image, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m)\n",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36madjust_tint\u001b[1;34m(image, tint_color)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madjust_tint\u001b[39m(image, tint_color):\n\u001b[1;32m---> 31\u001b[0m     adjusted_image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddWeighted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtint_color\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m adjusted_image\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\arithm.cpp:652: error: (-215:Assertion failed) type2 == CV_64F && (sz2.height == 1 || sz2.height == 4) in function 'cv::arithm_op'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def equalize_channels(image, num_channels):\n",
    "    if image.shape[2] == num_channels:\n",
    "        return image\n",
    "    elif image.shape[2] > num_channels:\n",
    "        return image[:, :, :num_channels]\n",
    "    else:\n",
    "        channels_diff = num_channels - image.shape[2]\n",
    "        last_channel = image[:, :, -1]\n",
    "        for _ in range(channels_diff):\n",
    "            image = np.concatenate((image, last_channel[:, :, np.newaxis]), axis=2)\n",
    "        return image\n",
    "\n",
    "def adjust_contrast(image, contrast):\n",
    "    adjusted_image = np.clip(image * contrast, 0, 255).astype(np.uint8)\n",
    "    return adjusted_image\n",
    "\n",
    "def adjust_brightness(image, brightness):\n",
    "    adjusted_image = np.clip(image + brightness, 0, 255).astype(np.uint8)\n",
    "    return adjusted_image\n",
    "\n",
    "def adjust_saturation(image, saturation):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hsv_image[:, :, 1] = np.clip(hsv_image[:, :, 1] * saturation, 0, 255)\n",
    "    adjusted_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2BGR)\n",
    "    return adjusted_image\n",
    "\n",
    "def adjust_tint(image, tint_color):\n",
    "    adjusted_image = cv2.addWeighted(image, 1, tint_color.astype(image.dtype), 1, 0)\n",
    "    return adjusted_image\n",
    "\n",
    "def adjust_temperature(image, temperature):\n",
    "    kelvin_table = {\n",
    "        1000: (255, 56, 0),\n",
    "        1500: (255, 109, 0),\n",
    "        2000: (255, 137, 18),\n",
    "        2500: (255, 161, 72),\n",
    "        3000: (255, 180, 107),\n",
    "        3500: (255, 196, 137),\n",
    "        4000: (255, 209, 163),\n",
    "        4500: (255, 219, 186),\n",
    "        5000: (255, 228, 206),\n",
    "        5500: (255, 236, 224),\n",
    "        6000: (255, 243, 239),\n",
    "        6500: (255, 249, 253),\n",
    "        7000: (245, 243, 255),\n",
    "        7500: (235, 238, 255),\n",
    "        8000: (227, 233, 255),\n",
    "        8500: (220, 229, 255),\n",
    "        9000: (214, 225, 255),\n",
    "        9500: (208, 222, 255),\n",
    "        10000: (204, 219, 255)\n",
    "    }\n",
    "    temperature = max(temperature, 1000)\n",
    "    temperature = min(temperature, 10000)\n",
    "    r, g, b = kelvin_table[temperature]\n",
    "    adjusted_image = np.stack((image[:, :, 0] * (r / 255), image[:, :, 1] * (g / 255), image[:, :, 2] * (b / 255)), axis=-1)\n",
    "    adjusted_image = np.clip(adjusted_image, 0, 255).astype(np.uint8)\n",
    "    return adjusted_image\n",
    "\n",
    "def apply_vignette(image, intensity):\n",
    "    height, width = image.shape[:2]\n",
    "    mask = np.zeros((height, width), dtype=np.float32)\n",
    "    cv2.circle(mask, (int(width/2), int(height/2)), int(min(width, height) * 0.7), 1, -1)\n",
    "    mask = cv2.GaussianBlur(mask, (0, 0), sigmaX=width/2, sigmaY=height/2)\n",
    "    result = cv2.multiply(image, mask[..., np.newaxis])\n",
    "    return result\n",
    "\n",
    "def color_transfer(input_image_path, target_image_path):\n",
    "    # Load the input and target images\n",
    "    input_image = cv2.imread(input_image_path)\n",
    "    target_image = cv2.imread(target_image_path)\n",
    "\n",
    "    # Equalize the number of channels in the images\n",
    "    num_channels = max(input_image.shape[2], target_image.shape[2])\n",
    "    input_image = equalize_channels(input_image, num_channels)\n",
    "    target_image = equalize_channels(target_image, num_channels)\n",
    "\n",
    "    # Resize the input image to match the size of the target image\n",
    "    input_image = cv2.resize(input_image, (target_image.shape[1], target_image.shape[0]))\n",
    "\n",
    "    # Adjust the contrast\n",
    "    contrast = 1.2  # Adjust the contrast factor as desired\n",
    "    adjusted_image = adjust_contrast(input_image, contrast)\n",
    "\n",
    "    # Adjust the brightness\n",
    "    brightness = 20  # Adjust the brightness factor as desired\n",
    "    adjusted_image = adjust_brightness(adjusted_image, brightness)\n",
    "\n",
    "    # Adjust the saturation\n",
    "    saturation = 1.5  # Adjust the saturation factor as desired\n",
    "    adjusted_image = adjust_saturation(adjusted_image, saturation)\n",
    "\n",
    "    # Tint Adjustment\n",
    "    tint_color = np.array([50, 0, 0], dtype=np.float32)  # Adjust the tint color as desired\n",
    "    adjusted_image = adjust_tint(adjusted_image, tint_color)\n",
    "\n",
    "    # Temperature Adjustment\n",
    "    adjusted_image = adjust_temperature(adjusted_image, temperature=1.2)\n",
    "\n",
    "    # Apply vignette effect\n",
    "    vignette_intensity = 0.6  # Adjust the intensity of the vignette effect as desired\n",
    "    adjusted_image = apply_vignette(adjusted_image, vignette_intensity)\n",
    "\n",
    "    # Display the results\n",
    "    cv2.imshow(\"Color Transfer Result\", adjusted_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Save the results\n",
    "    # ...\n",
    "\n",
    "# Provide the paths to your input and target images\n",
    "input_image_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\FACE MATCHING DATA SET\\DSC_2498.JPG\"\n",
    "target_image_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\FACE MATCHING DATA SET\\DSC_2499.JPG\"\n",
    "\n",
    "# Call the color transfer function\n",
    "color_transfer(input_image_path, target_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7c89805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def color_transfer(input_image, target_image):\n",
    "    # Convert the input and target images to the Lab color space\n",
    "    input_lab = cv2.cvtColor(input_image, cv2.COLOR_BGR2Lab)\n",
    "    target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2Lab)\n",
    "\n",
    "    # Calculate the mean and standard deviation of the Lab channels for both images\n",
    "    input_mean, input_std = cv2.meanStdDev(input_lab)\n",
    "    target_mean, target_std = cv2.meanStdDev(target_lab)\n",
    "\n",
    "    # Adjust the mean and standard deviation of the target image to match the input image\n",
    "    adjusted_target_mean = np.squeeze(target_mean)  # Remove unnecessary dimensions\n",
    "    adjusted_target_std = input_std / target_std\n",
    "\n",
    "    # Apply the color transfer using the adjusted mean and standard deviation\n",
    "    adjusted_target_lab = (target_lab - target_mean) * adjusted_target_std + adjusted_target_mean\n",
    "\n",
    "    # Convert the adjusted Lab image back to the BGR color space\n",
    "    adjusted_target_bgr = cv2.cvtColor(adjusted_target_lab.astype(np.uint8), cv2.COLOR_Lab2BGR)\n",
    "\n",
    "    return adjusted_target_bgr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25487ce6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4288,2848,3) (3,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m target_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mMAHESH\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSRFP INTERNSHIP\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtrain_in_1\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFACE MATCHING DATA SET\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDSC_2499.JPG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Perform color transfer\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m result_image \u001b[38;5;241m=\u001b[39m \u001b[43mcolor_transfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[0;32m      9\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Image\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_image)\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mcolor_transfer\u001b[1;34m(input_image, target_image)\u001b[0m\n\u001b[0;32m     15\u001b[0m adjusted_target_std \u001b[38;5;241m=\u001b[39m input_std \u001b[38;5;241m/\u001b[39m target_std\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Apply the color transfer using the adjusted mean and standard deviation\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m adjusted_target_lab \u001b[38;5;241m=\u001b[39m (\u001b[43mtarget_lab\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_mean\u001b[49m) \u001b[38;5;241m*\u001b[39m adjusted_target_std \u001b[38;5;241m+\u001b[39m adjusted_target_mean\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Convert the adjusted Lab image back to the BGR color space\u001b[39;00m\n\u001b[0;32m     21\u001b[0m adjusted_target_bgr \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(adjusted_target_lab\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8), cv2\u001b[38;5;241m.\u001b[39mCOLOR_Lab2BGR)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4288,2848,3) (3,1) "
     ]
    }
   ],
   "source": [
    "# Load the input and target images\n",
    "input_image = cv2.imread(r\"C:\\\\Users\\\\MAHESH\\\\Desktop\\\\SRFP INTERNSHIP\\\\train_in_1\\\\FACE MATCHING DATA SET\\\\DSC_2498.JPG\")\n",
    "target_image = cv2.imread(r\"C:\\\\Users\\\\MAHESH\\\\Desktop\\\\SRFP INTERNSHIP\\\\train_in_1\\\\FACE MATCHING DATA SET\\\\DSC_2499.JPG\")\n",
    "\n",
    "# Perform color transfer\n",
    "result_image = color_transfer(input_image, target_image)\n",
    "\n",
    "# Display the results\n",
    "cv2.imshow(\"Input Image\", input_image)\n",
    "cv2.imshow(\"Target Image\", target_image)\n",
    "cv2.imshow(\"Color Transfer Result\", result_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64512550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the style image and the content image\n",
    "style_image_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2478.JPG\"  # Replace with the path to the style image\n",
    "content_image_path = r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Done\\DSC_2479.jpg\" # Replace with the path to the content image\n",
    "\n",
    "style_image = tf.keras.preprocessing.image.load_img(style_image_path)\n",
    "content_image = tf.keras.preprocessing.image.load_img(content_image_path)\n",
    "\n",
    "# Convert the images to arrays\n",
    "style_array = tf.keras.preprocessing.image.img_to_array(style_image)\n",
    "content_array = tf.keras.preprocessing.image.img_to_array(content_image)\n",
    "\n",
    "# Reshape the style and content arrays to match the expected input shape\n",
    "style_array = np.expand_dims(style_array, axis=0)\n",
    "content_array = np.expand_dims(content_array, axis=0)\n",
    "\n",
    "# Preprocess the style and content arrays\n",
    "style_array = tf.keras.applications.vgg19.preprocess_input(style_array)\n",
    "content_array = tf.keras.applications.vgg19.preprocess_input(content_array)\n",
    "\n",
    "# Define the VGG19 model\n",
    "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "\n",
    "# Specify the layers for style and content representation\n",
    "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "content_layer = 'block5_conv2'\n",
    "\n",
    "# Extract the style and content features\n",
    "style_outputs = [vgg.get_layer(layer).output for layer in style_layers]\n",
    "content_output = vgg.get_layer(content_layer).output\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs=vgg.input, outputs=[style_outputs, content_output])\n",
    "\n",
    "# Calculate the style and content features of the style and content images\n",
    "style_features = model(style_array)\n",
    "content_features = model(content_array)\n",
    "\n",
    "# Rest of the code remains the same\n",
    "# ...\n",
    "\n",
    "\n",
    "# Calculate the style and content features of the style and content images\n",
    "style_features = model(style_array)\n",
    "content_features = model(content_array)\n",
    "\n",
    "# Define the style and content weights\n",
    "style_weight = 1e-2\n",
    "content_weight = 1e4\n",
    "\n",
    "# Define the initial image as a variable to optimize\n",
    "init_image = tf.Variable(content_array)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
    "\n",
    "@tf.function()\n",
    "def style_transfer_step(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(image)\n",
    "        style_outputs = outputs[:-1]\n",
    "        content_output = outputs[-1]\n",
    "\n",
    "        style_loss = tf.add_n([tf.reduce_mean((style_output - target_style) ** 2)\n",
    "                               for style_output, target_style in zip(style_outputs, style_features)])\n",
    "        style_loss *= style_weight / len(style_layers)\n",
    "\n",
    "        content_loss = tf.reduce_mean((content_output - content_features) ** 2)\n",
    "        content_loss *= content_weight\n",
    "\n",
    "        total_loss = style_loss + content_loss\n",
    "\n",
    "    gradients = tape.gradient(total_loss, image)\n",
    "    optimizer.apply_gradients([(gradients, image)])\n",
    "    image.assign(tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=255.0))\n",
    "\n",
    "# Perform style transfer iterations\n",
    "iterations = 1000\n",
    "for i in range(1, iterations + 1):\n",
    "    style_transfer_step(init_image)\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i}/{iterations} complete.\")\n",
    "\n",
    "# Convert the final image array to a PIL image\n",
    "final_image = tf.keras.preprocessing.image.array_to_img(init_image.numpy()[0])\n",
    "\n",
    "# Display the final image\n",
    "plt.imshow(final_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Save the final image\n",
    "final_image.save(r\"C:\\\\Users\\\\MAHESH\\\\Desktop\\\\SRFP INTERNSHIP\\\\train_in_1\\\\FACE MATCHING DATA SET\\\\DSC_2499none.JPG\")  # Replace with the path to save the final image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a1d1470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#important\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the first image (reference image)\n",
    "reference_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2478.JPG\")\n",
    "\n",
    "# Load the second image (target image)\n",
    "target_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Done\\DSC_2479.jpg\")\n",
    "\n",
    "\n",
    "# Convert images to LAB color space\n",
    "reference_lab = cv2.cvtColor(reference_image, cv2.COLOR_BGR2LAB)\n",
    "target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "# Split LAB images into L, A, B channels\n",
    "reference_l, reference_a, reference_b = cv2.split(reference_lab)\n",
    "target_l, target_a, target_b = cv2.split(target_lab)\n",
    "\n",
    "# Create a CLAHE object (with default parameters or adjust as desired)\n",
    "clahe = cv2.createCLAHE()\n",
    "\n",
    "# Set the clip limit for CLAHE\n",
    "clahe.setClipLimit(2.0)\n",
    "\n",
    "# Apply CLAHE on the L channel of the target image using the L channel of the reference image as the contrast limit\n",
    "enhanced_l = clahe.apply(target_l)\n",
    "\n",
    "# Merge the enhanced L channel with the A and B channels of the target image\n",
    "enhanced_lab = cv2.merge([enhanced_l, target_a, target_b])\n",
    "\n",
    "# Convert the enhanced LAB image back to BGR color\n",
    "enhanced_image = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# Display the original target image and the enhanced image\n",
    "cv2.imshow('Original Target Image', target_image)\n",
    "cv2.imshow('Enhanced Image', enhanced_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4dd96ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#important\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the first image (reference image)\n",
    "reference_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Done\\DSC03694.jpg\")\n",
    "\n",
    "# Load the second image (target image)\n",
    "target_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\FACE MATCHING DATA SET\\PREDICTED\\MINI CODES\\banda babu enhance.png\")\n",
    "\n",
    "# Convert images to LAB color space\n",
    "reference_lab = cv2.cvtColor(reference_image, cv2.COLOR_BGR2LAB)\n",
    "target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "# Split LAB images into L, A, B channels\n",
    "reference_l, reference_a, reference_b = cv2.split(reference_lab)\n",
    "target_l, target_a, target_b = cv2.split(target_lab)\n",
    "\n",
    "# Create a CLAHE object (with default parameters or adjust as desired)\n",
    "clahe = cv2.createCLAHE()\n",
    "\n",
    "# Set the clip limit for CLAHE\n",
    "clahe.setClipLimit(2.0)\n",
    "\n",
    "# Apply CLAHE on the L channel of the target image using the L channel of the reference image as the contrast limit\n",
    "enhanced_l = clahe.apply(target_l)\n",
    "\n",
    "# Merge the enhanced L channel with the A and B channels of the target image\n",
    "enhanced_lab = cv2.merge([enhanced_l, target_a, target_b])\n",
    "\n",
    "# Convert the enhanced LAB image back to BGR color\n",
    "enhanced_image = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# Display the original target image and the enhanced image\n",
    "cv2.imshow('Original Target Image', target_image)\n",
    "cv2.imshow('Enhanced Image', enhanced_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b73780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#power lawww\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the first image (reference image)\n",
    "reference_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Done\\DSC_2479.jpg\")\n",
    "\n",
    "# Load the second image (target image)\n",
    "target_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2478.JPG\")\n",
    "\n",
    "# Convert images to LAB color space\n",
    "reference_lab = cv2.cvtColor(reference_image, cv2.COLOR_BGR2LAB)\n",
    "target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "# Split LAB images into L, A, B channels\n",
    "reference_l, reference_a, reference_b = cv2.split(reference_lab)\n",
    "target_l, target_a, target_b = cv2.split(target_lab)\n",
    "\n",
    "# Define the gamma value for power-law gamma correction\n",
    "gamma = 0.5\n",
    "\n",
    "# Apply power-law gamma correction to the L channel of the target image\n",
    "enhanced_l = np.power(target_l / 255.0, gamma) * 255.0\n",
    "enhanced_l = np.uint8(enhanced_l)\n",
    "\n",
    "# Merge the enhanced L channel with the A and B channels of the target image\n",
    "enhanced_lab = cv2.merge([enhanced_l, target_a, target_b])\n",
    "\n",
    "# Convert the enhanced LAB image back to BGR color\n",
    "enhanced_image = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# Display the original target image and the enhanced image\n",
    "cv2.imshow('Original Target Image', target_image)\n",
    "cv2.imshow('Enhanced Image', enhanced_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bdc1b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11692edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the first image (reference image)\n",
    "reference_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Done\\DSC_2479.jpg\")\n",
    "\n",
    "# Load the second image (target image)\n",
    "target_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2478.JPG\")\n",
    "\n",
    "# Convert images to LAB color space\n",
    "reference_lab = cv2.cvtColor(reference_image, cv2.COLOR_BGR2LAB)\n",
    "target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "# Split LAB images into L, A, B channels\n",
    "reference_l, reference_a, reference_b = cv2.split(reference_lab)\n",
    "target_l, target_a, target_b = cv2.split(target_lab)\n",
    "\n",
    "# Create a CLAHE object (with default parameters or adjust as desired)\n",
    "clahe = cv2.createCLAHE()\n",
    "\n",
    "# Set the clip limit for CLAHE\n",
    "clahe.setClipLimit(2.0)\n",
    "\n",
    "# Apply CLAHE on the L channel of the target image using the L channel of the reference image as the contrast limit\n",
    "enhanced_l_clahe = clahe.apply(target_l)\n",
    "\n",
    "# Define the gamma value for power-law gamma correction\n",
    "gamma = 0.5\n",
    "\n",
    "# Apply power-law gamma correction to the enhanced L channel\n",
    "enhanced_l_powerlaw = np.power(enhanced_l_clahe / 255.0, gamma) * 255.0\n",
    "enhanced_l_powerlaw = np.uint8(enhanced_l_powerlaw)\n",
    "\n",
    "# Merge the enhanced L channel with the A and B channels of the target image\n",
    "enhanced_lab = cv2.merge([enhanced_l_powerlaw, target_a, target_b])\n",
    "\n",
    "# Convert the enhanced LAB image back to BGR color\n",
    "enhanced_image = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# Display the original target image and the enhanced image\n",
    "cv2.imshow('Original Target Image', target_image)\n",
    "cv2.imshow('Enhanced Image', enhanced_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0efb4623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the first image (reference image)\n",
    "reference_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2478.JPG\")\n",
    "\n",
    "# Load the second image (target image)\n",
    "target_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Done\\DSC_2479.jpg\")\n",
    "\n",
    "# Convert images to HLS color space\n",
    "reference_hls = cv2.cvtColor(reference_image, cv2.COLOR_BGR2HLS)\n",
    "target_hls = cv2.cvtColor(target_image, cv2.COLOR_BGR2HLS)\n",
    "\n",
    "# Split HLS images into H, L, S channels\n",
    "reference_h, reference_l, reference_s = cv2.split(reference_hls)\n",
    "target_h, target_l, target_s = cv2.split(target_hls)\n",
    "\n",
    "# Create a CLAHE object (with default parameters or adjust as desired)\n",
    "clahe = cv2.createCLAHE()\n",
    "\n",
    "# Set the clip limit for CLAHE\n",
    "clahe.setClipLimit(2.0)\n",
    "\n",
    "# Apply CLAHE on the S channel of the target image using the S channel of the reference image as the contrast limit\n",
    "enhanced_s_clahe = clahe.apply(target_s)\n",
    "\n",
    "# Define the gamma value for power-law gamma correction\n",
    "gamma = 0.5\n",
    "\n",
    "# Apply power-law gamma correction to the enhanced S channel\n",
    "enhanced_s_powerlaw = np.power(enhanced_s_clahe / 255.0, gamma) * 255.0\n",
    "enhanced_s_powerlaw = np.uint8(enhanced_s_powerlaw)\n",
    "\n",
    "# Merge the enhanced S channel with the H and L channels of the target image\n",
    "enhanced_hls = cv2.merge([target_h, target_l, enhanced_s_powerlaw])\n",
    "\n",
    "# Convert the enhanced HLS image back to BGR color\n",
    "enhanced_image = cv2.cvtColor(enhanced_hls, cv2.COLOR_HLS2BGR)\n",
    "\n",
    "# Display the original target image and the enhanced image\n",
    "cv2.imshow('Original Target Image', target_image)\n",
    "cv2.imshow('Enhanced Image', enhanced_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cfa11c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m target_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_image.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Convert images to LAB color space\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m reference_lab \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2LAB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m target_lab \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(target_image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2LAB)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Split LAB images into L, A, B channels\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the first image (reference image)\n",
    "reference_image = cv2.imread('reference_image.jpg')\n",
    "\n",
    "# Load the second image (target image)\n",
    "target_image = cv2.imread('target_image.jpg')\n",
    "\n",
    "# Convert images to LAB color space\n",
    "reference_lab = cv2.cvtColor(reference_image, cv2.COLOR_BGR2LAB)\n",
    "target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "# Split LAB images into L, A, B channels\n",
    "reference_l, reference_a, reference_b = cv2.split(reference_lab)\n",
    "target_l, target_a, target_b = cv2.split(target_lab)\n",
    "\n",
    "# Create a CLAHE object (with default parameters or adjust as desired)\n",
    "clahe = cv2.createCLAHE()\n",
    "\n",
    "# Set the clip limit for CLAHE\n",
    "clahe.setClipLimit(2.0)\n",
    "\n",
    "# Apply CLAHE on the A channel of the target image using the A channel of the reference image as the contrast limit\n",
    "enhanced_a_clahe = clahe.apply(target_a)\n",
    "\n",
    "# Define the gamma value for power-law gamma correction\n",
    "gamma = 0.5\n",
    "\n",
    "# Apply power-law gamma correction to the enhanced A channel\n",
    "enhanced_a_powerlaw = np.power(enhanced_a_clahe / 255.0, gamma) * 255.0\n",
    "enhanced_a_powerlaw = np.uint8(enhanced_a_powerlaw)\n",
    "\n",
    "# Merge the enhanced A channel with the L and B channels of the target image\n",
    "enhanced_lab = cv2.merge([target_l, enhanced_a_powerlaw, target_b])\n",
    "\n",
    "# Convert the enhanced LAB image back to BGR color\n",
    "enhanced_image = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# Display the original target image and the enhanced image\n",
    "cv2.imshow('Original Target Image', target_image)\n",
    "cv2.imshow('Enhanced Image', enhanced_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ae0eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the first image (reference image)\n",
    "reference_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Done\\DSC_2479.jpg\")\n",
    "\n",
    "# Load the second image (target image)\n",
    "target_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2478.JPG\")\n",
    "\n",
    "# Convert images to LAB color space\n",
    "reference_lab = cv2.cvtColor(reference_image, cv2.COLOR_BGR2LAB)\n",
    "target_lab = cv2.cvtColor(target_image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "# Split LAB images into L, A, B channels\n",
    "reference_l, reference_a, reference_b = cv2.split(reference_lab)\n",
    "target_l, target_a, target_b = cv2.split(target_lab)\n",
    "\n",
    "# Create a CLAHE object (with default parameters or adjust as desired)\n",
    "clahe = cv2.createCLAHE()\n",
    "\n",
    "# Set the clip limit for CLAHE\n",
    "clahe.setClipLimit(2.0)\n",
    "\n",
    "# Apply CLAHE on the L channel of the target image using the L channel of the reference image as the contrast limit\n",
    "enhanced_l_clahe = clahe.apply(target_l)\n",
    "\n",
    "# Define the gamma value for power-law gamma correction\n",
    "gamma = 0.5\n",
    "\n",
    "# Apply power-law gamma correction to the enhanced L channel\n",
    "enhanced_l_powerlaw = np.power(enhanced_l_clahe / 255.0, gamma) * 255.0\n",
    "enhanced_l_powerlaw = np.uint8(enhanced_l_powerlaw)\n",
    "\n",
    "# Apply CLAHE on the S channel of the target image using the S channel of the reference image as the contrast limit\n",
    "enhanced_s_clahe = clahe.apply(target_a)\n",
    "\n",
    "# Apply power-law gamma correction to the enhanced S channel\n",
    "enhanced_s_powerlaw = np.power(enhanced_s_clahe / 255.0, gamma) * 255.0\n",
    "enhanced_s_powerlaw = np.uint8(enhanced_s_powerlaw)\n",
    "\n",
    "# Apply CLAHE on the A channel of the target image using the A channel of the reference image as the contrast limit\n",
    "enhanced_a_clahe = clahe.apply(target_a)\n",
    "\n",
    "# Apply power-law gamma correction to the enhanced A channel\n",
    "enhanced_a_powerlaw = np.power(enhanced_a_clahe / 255.0, gamma) * 255.0\n",
    "enhanced_a_powerlaw = np.uint8(enhanced_a_powerlaw)\n",
    "\n",
    "# Merge the enhanced L, S, and A channels with the B channel of the target image\n",
    "enhanced_lab = cv2.merge([enhanced_l_powerlaw, enhanced_s_powerlaw, enhanced_a_powerlaw])\n",
    "\n",
    "# Convert the enhanced LAB image back to BGR color\n",
    "enhanced_image = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# Display the original target image and the enhanced image\n",
    "cv2.imshow('Original Target Image', target_image)\n",
    "cv2.imshow('Enhanced Image', enhanced_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "26fe6666",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\arithm.cpp:652: error: (-215:Assertion failed) type2 == CV_64F && (sz2.height == 1 || sz2.height == 4) in function 'cv::arithm_op'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m red_shade \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Add red shade to the target image\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m target_image_red \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mred_shade\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Convert images to LAB color space\u001b[39;00m\n\u001b[0;32m     17\u001b[0m reference_lab \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(reference_image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2LAB)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\arithm.cpp:652: error: (-215:Assertion failed) type2 == CV_64F && (sz2.height == 1 || sz2.height == 4) in function 'cv::arithm_op'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the first image (reference image)\n",
    "reference_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Done\\DSC_2479.jpg\")\n",
    "\n",
    "# Load the second image (target image)\n",
    "target_image = cv2.imread(r\"C:\\Users\\MAHESH\\Desktop\\SRFP INTERNSHIP\\train_in_1\\Raw\\DSC_2478.JPG\")\n",
    "\n",
    "# Define the red shade color\n",
    "red_shade = np.array([0, 0, 255], dtype=np.uint8)\n",
    "\n",
    "# Add red shade to the target image\n",
    "target_image_red = cv2.add(target_image, red_shade)\n",
    "\n",
    "# Convert images to LAB color space\n",
    "reference_lab = cv2.cvtColor(reference_image, cv2.COLOR_BGR2LAB)\n",
    "target_lab_red = cv2.cvtColor(target_image_red, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "# Split LAB images into L, A, B channels\n",
    "reference_l, reference_a, reference_b = cv2.split(reference_lab)\n",
    "target_l_red, target_a_red, target_b_red = cv2.split(target_lab_red)\n",
    "\n",
    "# Create a CLAHE object (with default parameters or adjust as desired)\n",
    "clahe = cv2.createCLAHE()\n",
    "\n",
    "# Set the clip limit for CLAHE\n",
    "clahe.setClipLimit(2.0)\n",
    "\n",
    "# Apply CLAHE on the L channel of the target image using the L channel of the reference image as the contrast limit\n",
    "enhanced_l_clahe = clahe.apply(target_l_red)\n",
    "\n",
    "# Define the gamma value for power-law gamma correction\n",
    "gamma = 0.5\n",
    "\n",
    "# Apply power-law gamma correction to the enhanced L channel\n",
    "enhanced_l_powerlaw = np.power(enhanced_l_clahe / 255.0, gamma) * 255.0\n",
    "enhanced_l_powerlaw = np.uint8(enhanced_l_powerlaw)\n",
    "\n",
    "# Apply CLAHE on the A channel of the target image using the A channel of the reference image as the contrast limit\n",
    "enhanced_a_clahe = clahe.apply(target_a_red)\n",
    "\n",
    "# Apply power-law gamma correction to the enhanced A channel\n",
    "enhanced_a_powerlaw = np.power(enhanced_a_clahe / 255.0, gamma) * 255.0\n",
    "enhanced_a_powerlaw = np.uint8(enhanced_a_powerlaw)\n",
    "\n",
    "# Merge the enhanced L, A, and B channels\n",
    "enhanced_lab = cv2.merge([enhanced_l_powerlaw, enhanced_a_powerlaw, target_b_red])\n",
    "\n",
    "# Convert the enhanced LAB image back to BGR color\n",
    "enhanced_image = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# Display the original target image and the enhanced image\n",
    "cv2.imshow('Original Target Image', target_image)\n",
    "cv2.imshow('Enhanced Image', enhanced_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960bbbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
